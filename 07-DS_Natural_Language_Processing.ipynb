{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Application\n",
    "\n",
    "This notebook is intended to analyze the similarities and differences of 2020 special reports by NCSES. The analysis my lead to avoid duplication and create collaboration.  This notebook goes through a necessary step of any data science project - data cleaning. Data cleaning is a time consuming and unenjoyable task, yet it's a very important one. Keep in mind, \"garbage in, garbage out\". Feeding data that is not processed properly into a model will give us results that are meaningless.\n",
    "\n",
    "##  1) Data Pre-processing\n",
    "\n",
    "1.1. Getting the data - Scraping data from NCSES website \n",
    "\n",
    "1.2. Cleaning the data - Apply text pre-processing techniques\n",
    "\n",
    "1.3. Organizing the data - Organize the cleaned data into a way that is easy to input into other algorithms\n",
    "\n",
    "The output of this stage  will be clean, organized data in two standard text formats:\n",
    "\n",
    "1. **Corpus** - a collection of text\n",
    "2. **Document-Term Matrix** - word counts in matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping, pickle imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "# Scrapes transcript data from https://www.nsf.gov/statistics/publication-index.cfm\n",
    "def url_to_transcript(url):\n",
    "    '''Returns transcript data specifically from https://www.nsf.gov/statistics/publication-index.cfm.'''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = [p.text for p in soup.find().find_all('p')]\n",
    "    print(url)\n",
    "    return text\n",
    "\n",
    "# URLs of executive summaries of special reports\n",
    "urls = ['https://ncses.nsf.gov/pubs/nsb20201/executive-summary/',\n",
    "        'https://ncses.nsf.gov/pubs/nsb20202/executive-summary/',\n",
    "        'https://ncses.nsf.gov/pubs/nsb20203/executive-summary/',\n",
    "        'https://ncses.nsf.gov/pubs/nsb20204/executive-summary/',\n",
    "        'https://ncses.nsf.gov/pubs/nsb20205/executive-summary/',\n",
    "        'https://ncses.nsf.gov/pubs/nsb20206/executive-summary/',\n",
    "        'https://ncses.nsf.gov/pubs/nsb20207/executive-summary/']\n",
    "\n",
    "# CSpecial reports  names\n",
    "SR = ['NSB-2020-1', 'NSB-2020-2', 'NSB-2020-3', 'NSB-2020-4', 'NSB-2020-5','NSB-2020-6', 'NSB-2020-7']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping, pickle imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "# Scrapes transcript data from https://www.nsf.gov/statistics/publication-index.cfm\n",
    "def url_to_transcript(url):\n",
    "    '''Returns transcript data specifically from https://www.nsf.gov/statistics/publication-index.cfm.'''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = [p.text for p in soup.find().find_all('p')]\n",
    "    print(url)\n",
    "    return text\n",
    "\n",
    "# URLs of executive summaries of special reports\n",
    "urls = ['https://www.nsf.gov/awardsearch/showAward?AWD_ID=1849735&HistoricalAwards=false',\n",
    "        'https://www.nsf.gov/awardsearch/showAward?AWD_ID=2050833&HistoricalAwards=false']\n",
    "\n",
    "# CSpecial reports  names\n",
    "SR = ['NSB-2020-1', 'NSB-2020-2']\n",
    "\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually request transcripts (takes a few minutes to run)\n",
    "transcripts = [url_to_transcript(u) for u in urls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle files for later use\n",
    "\n",
    "# Make a new directory to hold the text files\n",
    "!mkdir transcripts\n",
    "\n",
    "for i, c in enumerate(SR):\n",
    "    with open(\"transcripts/\" + c + \".txt\", \"wb\") as file:\n",
    "        pickle.dump(transcripts[i], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled files---this helps to use saved files to reduce running\n",
    "data = {}\n",
    "for i, c in enumerate(SR):\n",
    "    with open(\"transcripts/\" + c + \".txt\", \"rb\") as file:\n",
    "        data[c] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check to make sure data has been loaded properly\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More checks\n",
    "data['NSB-2020-1'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for analysis\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the document-term matrix\n",
    "data = pd.read_pickle('../../Users/muluken/WorkingFiles/GitHUB_Notebooks/NLTK/nlp-in-python-tutorial/dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"../../Users/muluken/WorkingFiles/GitHUB_Notebooks/NLTK/nlp-in-python-tutorial/cv_stop.pkl\", \"rb\"))\n",
    "#id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
